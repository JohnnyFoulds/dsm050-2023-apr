{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-09 : Aspect Based Sentiment Analysis (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from time import sleep\n",
    "from typing import List, Dict, Tuple\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read local .env file\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the human feedback dataset\n",
    "df_source = pd.read_parquet('../../data/interim/01-06_human_classified.parquet')\n",
    "\n",
    "# show the data loaded\n",
    "print(df_source.shape)\n",
    "display(df_source.head(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_aspects = {\n",
    "    \"Billing & Payments\": [\n",
    "        \"Billing accuracy\",\n",
    "        \"Payment methods\",\n",
    "        \"Refunds/credits\",\n",
    "        \"Hidden charges\",\n",
    "        \"Monthly costs\"\n",
    "    ],\n",
    "    \"Network & Connectivity\": [\n",
    "        \"Signal strength\",\n",
    "        \"Network coverage\",\n",
    "        \"Data speeds (4G, 5G, etc.)\",\n",
    "        \"Call quality\",\n",
    "        \"Dropped calls\",\n",
    "        \"Roaming\"\n",
    "    ],\n",
    "    \"Customer Service\": [\n",
    "        \"Responsiveness\",\n",
    "        \"Friendliness/professionalism\",\n",
    "        \"Knowledge/competence\",\n",
    "        \"Resolution time\",\n",
    "        \"Availability (e.g., 24/7 support)\"\n",
    "    ],\n",
    "    \"Chatbots\": [\n",
    "        \"User-friendliness\",\n",
    "        \"Response accuracy\",\n",
    "        \"Speed of response\",\n",
    "        \"Ability to understand query\",\n",
    "        \"Escalation to human agents\"\n",
    "    ],\n",
    "    \"Account & Plans\": [\n",
    "        \"Account management (online portal/apps)\",\n",
    "        \"Plan flexibility\",\n",
    "        \"Plan pricing\",\n",
    "        \"Upgrade/downgrade process\",\n",
    "        \"Promotions and offers\"\n",
    "    ],\n",
    "    \"Hardware/Devices\": [\n",
    "        \"Setup/ease of installation\",\n",
    "        \"Device reliability\",\n",
    "        \"Device performance/speed\",\n",
    "        \"Rental vs. purchase options\",\n",
    "        \"Technical issues\"\n",
    "    ],\n",
    "    \"Value-added Services\": [\n",
    "        \"Quality of service\",\n",
    "        \"Pricing/value for money\",\n",
    "        \"Reliability\",\n",
    "        \"Content variety (for streaming)\",\n",
    "        \"Ease of use\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the entities and aspects as a string\n",
    "entities_aspects_str = json.dumps(entities_aspects, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of the output the llm should produce\n",
    "output_example = {\n",
    "    \"Customer Service\": {\n",
    "        \"Responsiveness\": \"Negative\",\n",
    "        \"Friendliness/professionalism\": \"Negative\",\n",
    "        \"Knowledge/competence\": \"Negative\",\n",
    "        \"Availability (e.g., 24/7 support)\": \"Negative\"\n",
    "    },\n",
    "    \"Chatbots\": {\n",
    "        \"User-friendliness\": \"Neutral\",\n",
    "        \"Response accuracy\": \"Negative\",\n",
    "        \"Ability to understand query\": \"Negative\"\n",
    "    }\n",
    "}\n",
    "\n",
    "output_example_str = json.dumps(output_example, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the template string\n",
    "template_string = \"\"\"\\\n",
    "Given the following entities and aspects per entity:\n",
    "\n",
    "```json\n",
    "{entities_aspects}\n",
    "```\n",
    "\n",
    "Please perform Aspect Based Sentiment Analysis on the following text:\n",
    "\n",
    "```text\n",
    "{text}\n",
    "```\n",
    "\n",
    "Only the JSON output is expected without any \"```json\" text surrounding it. Do not answer with anything except JSON. Only respond with entities and aspects present in the text.\n",
    "\n",
    "Output example:\n",
    "\n",
    "```json\n",
    "{output_example}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "prompt_template"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the complaint text\n",
    "review_title = df_source.loc[1, 'review_title']\n",
    "review_content = df_source.loc[1, 'review_content']\n",
    "complaint_text = f'# {review_title}\\n\\n{review_content}'\n",
    "\n",
    "pprint(complaint_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the prompt\n",
    "prompt = prompt_template.format_messages(\n",
    "    entities_aspects=entities_aspects_str,\n",
    "    text=complaint_text,\n",
    "    output_example=output_example_str\n",
    ")\n",
    "\n",
    "#print(prompt[0].content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_aspects(text:str,\n",
    "                     entities_aspects:str,\n",
    "                     output_example:str,\n",
    "                     prompt_template:ChatPromptTemplate) -> Dict:\n",
    "    \"\"\"Classify the aspects of a given text and return an output dictionary.\"\"\"\n",
    "    chat = ChatOpenAI(\n",
    "        temperature=0.0,\n",
    "        max_tokens=512,\n",
    "        model='gpt-4')\n",
    "\n",
    "    # create the prompt\n",
    "    prompt = prompt_template.format_messages(\n",
    "        entities_aspects=entities_aspects,\n",
    "        text=text,\n",
    "        output_example=output_example\n",
    "    )\n",
    "\n",
    "    # get the llm response\n",
    "    response = chat(prompt)\n",
    "\n",
    "    # return the result\n",
    "    return json.loads(response.content)\n",
    "\n",
    "# test the function\n",
    "aspects = classify_aspects(\n",
    "    text=complaint_text,\n",
    "    entities_aspects=entities_aspects_str,\n",
    "    output_example=output_example_str,\n",
    "    prompt_template=prompt_template)\n",
    "pprint(aspects)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_aspects(id:str, classification:Dict) -> Dict:\n",
    "    \"\"\"Flatten the aspects to be compatible with a dataframe and add the id as a column\"\"\"\n",
    "    flat_aspects = []\n",
    "\n",
    "    for entity, aspects in classification.items():\n",
    "        for aspect, polarity in aspects.items():\n",
    "            flat_aspects.append({\n",
    "                'id': id,\n",
    "                'entity': entity,\n",
    "                'aspect': aspect,\n",
    "                'polarity': polarity,\n",
    "            })\n",
    "\n",
    "    return flat_aspects\n",
    "\n",
    "# test the function\n",
    "flatten = flatten_aspects(\"12\", aspects)\n",
    "display(pd.DataFrame(flatten))\n",
    "#pprint(flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_dataset(\n",
    "        data:pd.DataFrame,\n",
    "        entities_aspects:str,\n",
    "        output_example:str,\n",
    "        prompt_template:ChatPromptTemplate,\n",
    "        id_column:str='id',\n",
    "        text_column:str='text') -> pd.DataFrame:\n",
    "    \"\"\"Classify the entire dataset using the given entities and aspects.\"\"\"\n",
    "    result = []\n",
    "\n",
    "    for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        # get the row data\n",
    "        id = row[id_column]\n",
    "        text = row[text_column]\n",
    "\n",
    "        # classify the text\n",
    "        classification = classify_aspects(\n",
    "            text=text,\n",
    "            entities_aspects=entities_aspects,\n",
    "            output_example=output_example,\n",
    "            prompt_template=prompt_template)\n",
    "        \n",
    "        result.extend(\n",
    "            flatten_aspects(id=id, classification=classification)\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "# test the function\n",
    "df_test = df_source.head(2)\n",
    "df_test['text'] = '# ' + df_source.review_title + '\\n\\n' + df_source.review_content\n",
    "\n",
    "df_result = classify_dataset(\n",
    "    data=df_test,\n",
    "    entities_aspects=entities_aspects_str,\n",
    "    output_example=output_example_str,\n",
    "    prompt_template=prompt_template,\n",
    "    id_column='id',\n",
    "    text_column='text')\n",
    "\n",
    "display(df_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the text column\n",
    "df_source['text'] = '# ' + df_source.review_title + '\\n\\n' + df_source.review_content\n",
    "\n",
    "# perform the classification\n",
    "df_result = classify_dataset(\n",
    "    data=df_source,\n",
    "    entities_aspects=entities_aspects_str,\n",
    "    output_example=output_example_str,\n",
    "    prompt_template=prompt_template,\n",
    "    id_column='id',\n",
    "    text_column='text')\n",
    "\n",
    "# save the results\n",
    "df_result.to_parquet('../../data/interim/01-09_absa.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
