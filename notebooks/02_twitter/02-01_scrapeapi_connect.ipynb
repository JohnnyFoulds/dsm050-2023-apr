{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-01 : Scrape API Connection\n",
    "\n",
    "Test scraping writter data using [Scrape API](scraperapi.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import List, Dict, Any\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_twitter(query:str, \n",
    "                   num:int, \n",
    "                   date_range_start:datetime,\n",
    "                   date_range_end:datetime) -> Dict:\n",
    "    \"\"\"Scrape Twitter for tweets matching query.\n",
    "\n",
    "    Args:\n",
    "        query: Query to search for.\n",
    "        num: Number of tweets to return.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary of tweets.\n",
    "    \"\"\"\n",
    "    scrape_url = 'https://api.scraperapi.com/structured/twitter/search'\n",
    "\n",
    "    # set the parameters\n",
    "    params = {\n",
    "        'api_key': os.getenv('SCRAPE_API_KEY'),\n",
    "        'query': query,\n",
    "        'num': num,\n",
    "        date_range_start: date_range_start.strftime('%Y-%m-%d'),\n",
    "        date_range_end: date_range_end.strftime('%Y-%m-%d')\n",
    "    }\n",
    "\n",
    "    # make the request\n",
    "    response = requests.get(scrape_url, params=params)\n",
    "\n",
    "    # return the response\n",
    "    return response.json()\n",
    "\n",
    "# test the api\n",
    "#response = scrape_twitter('vodacom tobi', 2, datetime(2023, 1, 1), datetime(2023, 1, 2))\n",
    "#pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweets(query:str, \n",
    "                   start_date:datetime,\n",
    "                   end_date:datetime,\n",
    "                   output_path:str,\n",
    "                   retry_count=3) -> None:\n",
    "    \n",
    "    # Loop backwards one day at a time\n",
    "    date_list = []\n",
    "    current_date = start_date\n",
    "    while current_date >= end_date:\n",
    "        date_list.append(current_date)\n",
    "        current_date -= timedelta(days=1)\n",
    "\n",
    "    # scrape the tweets\n",
    "    for current_date in tqdm(date_list):\n",
    "        retries = 0\n",
    "        while True:\n",
    "            try:        \n",
    "                tweets = scrape_twitter(\n",
    "                    query=query,\n",
    "                    num=1000,\n",
    "                    date_range_start=current_date,\n",
    "                    date_range_end=current_date + timedelta(days=1)),\n",
    "\n",
    "                # save the results as a json file\n",
    "                date_str = current_date.strftime('%Y-%m-%d')\n",
    "                with open(f'{output_path}/02-01_{date_str}.json', 'w') as f:\n",
    "                    json.dump(tweets, f)\n",
    "\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print('.', end='')\n",
    "                retries += 1\n",
    "                sleep(retries * 2)\n",
    "                if retries > retry_count:\n",
    "                    print(e)\n",
    "                    break\n",
    "\n",
    "# # test the function\n",
    "# query = '( (vodacom OR #vodacom OR @vodacom) AND (tobi OR #tobi or @tobi) )'\n",
    "# scrape_tweets(query='vodacom tobi', \n",
    "#               start_date=datetime(2023, 2, 12), \n",
    "#               end_date=datetime(2023, 2, 11),\n",
    "#               output_path='../../data/raw/twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344c0e1ddd5a45249ba905fbfa0c230e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......Expecting value: line 1 column 1 (char 0)\n",
      "....Expecting value: line 1 column 1 (char 0)\n",
      ".."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/requests/models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 971\u001b[0m     \u001b[39mreturn\u001b[39;00m complexjson\u001b[39m.\u001b[39;49mloads(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    972\u001b[0m \u001b[39mexcept\u001b[39;00m JSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[39m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[39m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mscrape_tweets\u001b[0;34m(query, start_date, end_date, output_path, retry_count)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mtry\u001b[39;00m:        \n\u001b[0;32m---> 19\u001b[0m     tweets \u001b[39m=\u001b[39m scrape_twitter(\n\u001b[1;32m     20\u001b[0m         query\u001b[39m=\u001b[39;49mquery,\n\u001b[1;32m     21\u001b[0m         num\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[1;32m     22\u001b[0m         date_range_start\u001b[39m=\u001b[39;49mcurrent_date,\n\u001b[1;32m     23\u001b[0m         date_range_end\u001b[39m=\u001b[39;49mcurrent_date \u001b[39m+\u001b[39;49m timedelta(days\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)),\n\u001b[1;32m     25\u001b[0m     \u001b[39m# save the results as a json file\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mscrape_twitter\u001b[0;34m(query, num, date_range_start, date_range_end)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m# return the response\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39;49mjson()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/requests/models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[39mexcept\u001b[39;00m JSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[39m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[39m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 975\u001b[0m     \u001b[39mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[39m.\u001b[39mmsg, e\u001b[39m.\u001b[39mdoc, e\u001b[39m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scrape_tweets(query\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mvodacom tobi\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m      2\u001b[0m               start_date\u001b[39m=\u001b[39;49mdatetime(\u001b[39m2023\u001b[39;49m, \u001b[39m7\u001b[39;49m, \u001b[39m31\u001b[39;49m), \n\u001b[1;32m      3\u001b[0m               end_date\u001b[39m=\u001b[39;49mdatetime(\u001b[39m2021\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m),\n\u001b[1;32m      4\u001b[0m               output_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../../data/raw/twitter\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m, in \u001b[0;36mscrape_tweets\u001b[0;34m(query, start_date, end_date, output_path, retry_count)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m retries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 34\u001b[0m sleep(retries \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m retry_count:\n\u001b[1;32m     36\u001b[0m     \u001b[39mprint\u001b[39m(e)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scrape_tweets(query='vodacom tobi', \n",
    "              start_date=datetime(2023, 7, 31), \n",
    "              end_date=datetime(2021, 1, 1),c\n",
    "              output_path='../../data/raw/twitter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
