{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-01 : Scrape API Connection\n",
    "\n",
    "Test scraping writter data using [Scrape API](scraperapi.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import List, Dict, Any\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_twitter(query:str, \n",
    "                   num:int, \n",
    "                   date_range_start:datetime,\n",
    "                   date_range_end:datetime) -> Dict:\n",
    "    \"\"\"Scrape Twitter for tweets matching query.\n",
    "\n",
    "    Args:\n",
    "        query: Query to search for.\n",
    "        num: Number of tweets to return.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary of tweets.\n",
    "    \"\"\"\n",
    "    scrape_url = 'https://api.scraperapi.com/structured/twitter/search'\n",
    "\n",
    "    # set the parameters\n",
    "    params = {\n",
    "        'api_key': os.getenv('SCRAPE_API_KEY'),\n",
    "        'query': query,\n",
    "        'num': num,\n",
    "        date_range_start: date_range_start.strftime('%Y-%m-%d'),\n",
    "        date_range_end: date_range_end.strftime('%Y-%m-%d')\n",
    "    }\n",
    "\n",
    "    # make the request\n",
    "    response = requests.get(scrape_url, params=params)\n",
    "\n",
    "    # return the response\n",
    "    return response.json()\n",
    "\n",
    "# test the api\n",
    "#response = scrape_twitter('vodacom tobi', 2, datetime(2023, 1, 1), datetime(2023, 1, 2))\n",
    "#pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweets(query:str, \n",
    "                   start_date:datetime,\n",
    "                   end_date:datetime,\n",
    "                   output_path:str,\n",
    "                   retry_count=3) -> None:\n",
    "    \n",
    "    # Loop backwards one day at a time\n",
    "    date_list = []\n",
    "    current_date = start_date\n",
    "    while current_date >= end_date:\n",
    "        date_list.append(current_date)\n",
    "        current_date -= timedelta(days=1)\n",
    "\n",
    "    # scrape the tweets\n",
    "    for current_date in tqdm(date_list):\n",
    "        retries = 0\n",
    "        while True:\n",
    "            try:        \n",
    "                tweets = scrape_twitter(\n",
    "                    query=query,\n",
    "                    num=1000,\n",
    "                    date_range_start=current_date,\n",
    "                    date_range_end=current_date + timedelta(days=1)),\n",
    "\n",
    "                # save the results as a json file\n",
    "                date_str = current_date.strftime('%Y-%m-%d')\n",
    "                with open(f'{output_path}/02-01_{date_str}.json', 'w') as f:\n",
    "                    json.dump(tweets, f)\n",
    "\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print('.', end='')\n",
    "                retries += 1\n",
    "                sleep(retries * 2)\n",
    "                if retries > retry_count:\n",
    "                    print(e)\n",
    "                    break\n",
    "\n",
    "# # test the function\n",
    "# query = '( (vodacom OR #vodacom OR @vodacom) AND (tobi OR #tobi or @tobi) )'\n",
    "# scrape_tweets(query='vodacom tobi', \n",
    "#               start_date=datetime(2023, 2, 12), \n",
    "#               end_date=datetime(2023, 2, 11),\n",
    "#               output_path='../../data/raw/twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609fceeea4874c4f88769c967c2063b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scrape_tweets(query='vodacom tobi', \n",
    "              start_date=datetime(2023, 7, 31), \n",
    "              end_date=datetime(2021, 1, 1),\n",
    "              output_path='../../data/raw/twitter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
